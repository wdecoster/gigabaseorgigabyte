---
title: "Averaging basecall quality scores the right way"
date: 2017-06-26T12:00:07
draft: false
tags: ["NanoPlot", "Python"]
categories: ["Nanopore", "Plotting"]
---

For my [NanoPlot](https://github.com/wdecoster/NanoPlot) tool, I have been calculating the average basecall quality of a read by simply calculating the arithmetic mean of the Phred scores. I recently also added an option to generate plots based on the sequencing_summary.txt file generated by the albacore basecaller, which completely avoids parsing the fastq file and calculating the mean. But as you can see below this doesn't yield an equivalent result when plotting the log transformed read length versus average read quality. The first plot shows the quality as calculated from the fastq file, the second plot was generated using the albacore summary.  This difference in average quality was also observed by [Ola Wallerman](https://twitter.com/OWallerman).

![FASTQ-Log_LengthvsQualityScatterPlot_kde](/gigabaseorgigabyte/images/2017-06_fastq-log_lengthvsqualityscatterplot_kde.png)![SUMMARY-Log_LengthvsQualityScatterPlot_kde](/gigabaseorgigabyte/images/2017-06_summary-log_lengthvsqualityscatterplot_kde.png)

Forrest Brennen from Oxford Nanopore Technologies explained what's going wrong on the community forum:
What you'd like the mean_qscore for a read to represent is the mean error rate for that read, but this means you can't just do a simple arithmetic mean of all the qscores, because it won't be a representation of the mean error rate then.

For example, say you have three bases, with error rates of 1%, 10%, and 50% (and qscores of 20, 10, and ~3 respectively).

The arithmetic mean of the qscores is 33 / 3 = 11.
The arithmetic mean of the error rates is 0.61 / 3 ~= 0.2.
The qscore for the arithmetic mean of the error rates is -10 * Log10(0.2) = 6.99

If you take the qscore for each base, convert it back to an error probability, take the mean of those, and then convert that mean error back into a qscore, you should get something very close to what albacore provides.
Looks like I need to adapt my script! Initially, my function for averaging quality scores (in the [nanomath](https://github.com/wdecoster/nanomath) module) was simply the following:

https://gist.github.com/wdecoster/4a4a9b505bcb31093a9f4f4683c5186f

Adapting that to the one below resulted in the plot shown below the code, which indeed shrinks the quality scores but isn't entirely the same as the albacore average basecall quality, yet.

https://gist.github.com/wdecoster/63626825b203896d4672fc1446d974ba

![FIXED-Log_LengthvsQualityScatterPlot_kde](/gigabaseorgigabyte/images/2017-06_fixed-log_lengthvsqualityscatterplot_kde.png)

With this change in average read quality scores, I should revisit my conclusion on [the accuracy of the basecall quality scores](https://gigabaseorgigabyte.wordpress.com/2017/06/12/oxford-nanopore-basecall-quality-scores/). Now, for aligned reads, the peak average quality is about 13 which theoretically corresponds to an accuracy of 95%. Looking at the quality scores above from the albacore summary result the peak is just below 12, which would correspond to ~93% accuracy. Those numbers are getting closer to ~88%, which is obtained by calculating the percent identity from the alignment.

![PercentIdentityvsAverageBaseQuality_kde](/gigabaseorgigabyte/images/2018-08_percentidentityvsaveragebasequality_kde.png)